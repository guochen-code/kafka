local kafka
aws glue - stream schema registry

Avro Schema Used:
------------------------------------
{
  "namespace": "aws_schema_registry.integrationtests",
  "type": "record",
  "name": "User",
  "fields": [
    {"name": "name", "type": "string"},
    {"name": "Age", "type": "int"}
  ]
}


Python Code:
--------------------------
#pip3 install boto3 -t.
#pip3 install aws-glue-schema-registry --upgrade --use-pep517 -t .
#pip install kafka-python -t .
import boto3
from time import sleep
from json import dumps
from kafka import KafkaProducer
from aws_schema_registry import DataAndSchema, SchemaRegistryClient
from aws_schema_registry.avro import AvroSchema
from aws_schema_registry.adapter.kafka import KafkaSerializer

session = boto3.Session(aws_access_key_id='{}', aws_secret_access_key='{}')

glue_client = session.client('glue', region_name='us-east-1') # already created in us-east-1

*** whenever we use registry, we need two thing: 1) schema file; 2) Avro Serializer

# Create the schema registry client, which is a fa√ßade around the boto3 glue client
client = SchemaRegistryClient(glue_client,
                              registry_name='my-registry')

# Create the serializer
serializer = KafkaSerializer(client)

# Create the producer
producer = KafkaProducer(bootstrap_servers=['127.0.0.1:9092'],value_serializer=serializer)


# Our producer needs a schema to send along with the data.
# In this example we're using Avro, so we'll load an .avsc file.
with open('C:/Users/USER/PycharmProjects/kafka_python/user.avsc', 'r') as schema_file:
    schema = AvroSchema(schema_file.read())



# Send message data along with schema
data = {
    'name': 'Hello',
    'Age':45
}
#data={'Partiiton_no':2}
record_metadata =producer.send('glue_schema_bms', value=(data, schema)).get(timeout=10) 
# avro serializer do two things: 1) check compatibility between data and schema; 2) register schema in aws glue if it is used for the first time
# here we use synchronous sending approach
# producer will automatically create a topic with name of 'glue_schema_bms'
# in aws glue, you will find the registered schema has the same name as the topic name
print(record_metadata.topic)
print(record_metadata.partition)
print(record_metadata.offset)

********************************************************************************************************************************************
if new schema, producer will understand this schema got changed, will go to registry and schema registry will do a compatibility check with earlier version
if versions are compatible, it will register that in schema registry and return schema ID, avro serializer will add new ID with serialized data and 
send to kafka cluster
consumer will get new schema ID, local cache will not be available, so will go to registry and get updated schema, and use that to deserialize data

schema registry automaticall handles schema evolution

but if add a new field, it will fail, because it is not backward compatibility, should start with consumer not producer.
if you run python producer script, it will fail and if you go to aws glue, will find failed status for this registration.

if change the topic name in producer, it will create a new schema in aws glue, remember to change the schema file


